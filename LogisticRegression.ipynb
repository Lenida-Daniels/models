{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565c7441-2b6c-4347-8ec4-1becb58cf66b",
   "metadata": {},
   "source": [
    "# LogisticRegression\n",
    "\n",
    "- Logistic Regression is a classification algorithm, despite the name \"regression\".\n",
    "\n",
    "- It predicts probabilities using a sigmoid.\n",
    "\n",
    "- It predicts the probability that a given input belongs to a class.\n",
    "\n",
    "- Works well for binary classification (e.g., spam vs not-spam), but can also be extended to multiclass classification.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1aafc8-6846-4b08-8098-bf597d03e305",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a39540a-b3d4-4ee0-82b3-2cacc5408c53",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- When scikit-learn trains Logistic Regression, it needs to solve an optimisation problem (minimise a cost function).\n",
    "\n",
    "  **The Cost Function (Loss Function**)\n",
    "  \n",
    "  - To measure how good/bad our predictions are, we use a cost function.\n",
    "    \n",
    "  - For Logistic Regression, it’s usually the Log Loss (Cross-Entropy Loss)\n",
    "     - If prediction matches reality --> cost is low.\n",
    "\n",
    "     - If prediction is wrong with high confidence --> cost is very high.\n",
    "\n",
    "  **Optimisation Problem**\n",
    "  \n",
    "  - we need to find the values of w (weights (coefficients) we need to learn) and b (bias (intercept)) that minimise the cost function.\n",
    " \n",
    "  - Since there’s no simple formula to solve this, we rely on numerical optimisation methods (solvers).\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e62e197b-1c46-4f34-b560-dff01d1663ea",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2e812-833d-455b-8edb-cc42a3cca425",
   "metadata": {},
   "source": [
    "## Solvers\n",
    "\n",
    "- Scikit-learn uses different algorithms (solvers) to minimise the cost function:\n",
    "\n",
    "1. Gradient Descent (sag, saga) --> gradually adjust w and b by moving in the opposite direction of the gradient.\n",
    "\n",
    "2. Newton’s Method (newton-cg, lbfgs) --> uses second derivatives (Hessian matrix) for faster convergence.\n",
    "\n",
    "3. Coordinate Descent (liblinear) --> optimises one weight at a time.\n",
    "\n",
    " - Each solver has pros/cons depending on dataset size, number of features, and type of regularisation.\n",
    "\n",
    "**1. liblinear**\n",
    "\n",
    "- Good for small datasets.\n",
    "\n",
    "- Supports L1 and L2 regularisation.\n",
    "\n",
    "- Only supports binary classification directly.\n",
    "\n",
    "- For multiclass, you need One-vs-Rest (OvR): train one model per class vs all others.\n",
    "\n",
    "**lbfgs**\n",
    "\n",
    "- Handles L2 regularisation.\n",
    "\n",
    "- Works well for large datasets and multiclass problems.\n",
    "\n",
    "- Optimises the multinomial loss (a proper multiclass logistic regression formulation).\n",
    "\n",
    "**newton-cg**\n",
    "\n",
    "- Similar to lbfgs, also supports L2 regularisation and multinomial loss.\n",
    "\n",
    "- Slower, but very accurate.\n",
    "\n",
    "**sag (Stochastic Average Gradient)**\n",
    "\n",
    "- Fast for large datasets with many samples and features.\n",
    "\n",
    "- Supports L2 regularisation only.\n",
    "\n",
    "**saga**\n",
    "\n",
    "- Very flexible.\n",
    "\n",
    "- Supports L1, L2, and Elastic-Net regularisation.\n",
    "\n",
    "- Works for multiclass problems with multinomial loss.\n",
    "\n",
    "- Scales well for large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6eed552-5550-486f-9ac2-aa1974fb0d05",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af8005-82c4-41ea-b980-aa8184e3ce8f",
   "metadata": {},
   "source": [
    "## Overfitting\n",
    "\n",
    "Overfitting happens when a model learns too much detail from the training data, including noise and random fructuations, instead of just the true underlying patterns.\n",
    "\n",
    "So, the model performs very well on training data, but badly on new/unseen data (poor generalisation)\n",
    "\n",
    "- Think of it like memorising exam past papers word-for-word instead of understanding concepts, you’ll fail if the exam questions change slightly.\n",
    "\n",
    "**Signs of Overfitting**\n",
    "\n",
    "1. Training accuracy very high, test accuracy much lower.\n",
    "\n",
    "2. Coefficients are very large (model is too sensitive to features).\n",
    "\n",
    "3. Model reacts strongly to small variations in data.\n",
    "\n",
    "\n",
    "Overfitting is caused mainly by:\n",
    "\n",
    "1. Too many features vs too little data\n",
    "\n",
    "2. High model complexity\n",
    "\n",
    "3. Noisy or unclean data\n",
    "\n",
    "4. Training too long without control\n",
    "\n",
    "5. Lack of regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e8274-685c-4461-927f-afaca0d254e5",
   "metadata": {},
   "source": [
    "## Regularisation\n",
    "\n",
    "\n",
    "- To prevent overfitting, scikit-learn adds a penalty term to the cost function\n",
    "\n",
    "- This pushes coefficients to stay small (or zero in L1), which keeps the model simpler\n",
    "\n",
    "\n",
    "**L1 regularisation (Lasso)**\n",
    "\n",
    "- Encourages sparsity (some coefficients become exactly 0).\n",
    "\n",
    "- Useful for feature selection.\n",
    "\n",
    "**L2 regularisation (Ridge)**\n",
    "\n",
    "- Shrinks coefficients but doesn’t force them to 0.\n",
    "\n",
    "- Helps when features are correlated.\n",
    "\n",
    "**Elastic Net**\n",
    "\n",
    "- A mix of L1 and L2.\n",
    "\n",
    "- Only available with the saga solver.\n",
    "\n",
    " By default, scikit-learn applies L2 regularisation to logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf100b9-5392-4137-a854-76a2495f4377",
   "metadata": {},
   "source": [
    "C controls how strong the penalty is:\n",
    "\n",
    "- Small C --> stronger regularisation (more shrinking).\n",
    "\n",
    "- Large C --> weaker regularisation (behaves closer to no penalty)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daa067a-dca1-4dc7-8e0d-ce231c1338a4",
   "metadata": {},
   "source": [
    "## Multiclass Handling\n",
    "\n",
    "Logistic Regression is naturally binary.\n",
    "\n",
    "For multiclass:\n",
    "\n",
    "1. Multinomial loss (softmax) --> one model that directly predicts multiple classes.\n",
    "(Supported by lbfgs, newton-cg, sag, saga)\n",
    "\n",
    "2. One-vs-Rest (OvR) --> train one classifier per class vs all others.\n",
    "(Used by liblinear)\n",
    "\n",
    "Example:\n",
    "\n",
    "Classify flowers into 3 types (Setosa, Versicolor, Virginica)\n",
    "\n",
    "Multinomial: one model outputs probabilities for 3 classes.\n",
    "\n",
    "OvR: train 3 separate binary classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49868cb-875b-471d-9a50-cf478b621968",
   "metadata": {},
   "source": [
    "## class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='deprecated', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07a7750-ef40-48be-8cff-08f9c9099d48",
   "metadata": {},
   "source": [
    "## `What do these parameters mean?`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cdea6-5f85-4b46-98c7-86568521d677",
   "metadata": {},
   "source": [
    "- **penalty{‘l1’, ‘l2’, ‘elasticnet’, None}, default=’l2’**\n",
    "\n",
    "  - None: no penalty is added.\n",
    "\n",
    "  - 'l2': add a L2 penalty term and it is the default choice.\n",
    "\n",
    "  - 'l1': add a L1 penalty term.\n",
    "\n",
    "  - 'elasticnet': both L1 and L2 penalty terms are added.\n",
    "\n",
    "    **NOTE**\n",
    "    - Some penalties may not work with some solvers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7803c05f-2f88-40ff-b319-65f6946507ac",
   "metadata": {},
   "source": [
    "`*` - means every parameter after it should be a keyword argument,not be a positional argument."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2530e03c-4c9e-40a8-bb6a-3289f04de1f3",
   "metadata": {},
   "source": [
    "- dual : bool, default=False\n",
    "\n",
    "   - Dual (constrained) or primal (regularized)\n",
    "   - Dual formulation is only implemented for l2 penalty with liblinear solver.\n",
    "   - Prefer dual=False when n_samples > n_features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09424195-113d-4d68-80b7-b231ecc3f1be",
   "metadata": {},
   "source": [
    "- tol : float, default=1e-4\n",
    " \n",
    "   - Tolerance for stopping criteria.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e36b884-f259-4e18-8013-aef39567b0c5",
   "metadata": {},
   "source": [
    "- C : float, default=1.0\n",
    "   - Inverse of regularization strength; must be a positive float. Like in support vector machines, smaller values specify stronger regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939216bd-7ba6-4c8c-9cba-15651da6dd49",
   "metadata": {},
   "source": [
    "- fit_intercept : bool, default=True\n",
    "\n",
    "  - Specifies if a constant (a.k.a. bias or intercept) should be added to the decision function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50bc522-b173-4a15-b07d-6d41687c5379",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
